{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmgW63Y1OihOH1dgf9ZF6Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BMugo84/pytorch_in_25_hours/blob/main/08_pytorch_paper_replicating.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Notes:  Reading Machine Learning Papers**\n",
        "\n",
        "**Paper:** An image is worth 16x16 words (Attention is all you need)\n",
        "\n",
        "**Transformers:** This is a deep-learning model that adopts the mechanism of self-attention, differentially weighing the significance of each part of input data (Wikipedia).\n",
        "\n",
        "**Attention:** This is the most vivid part of an image that captures your eyes and delivers the image as a whole. E.g., a dog in the grass by a tree under daylight.\n",
        "\n",
        "Attention uses selective focus, importance weighting, context dependence, relationships, and probabilistic models.\n",
        "\n",
        "**Sources:**\n",
        "- arxiv.org\n",
        "- AK-Twitter (@akallz)\n",
        "- vit-pytorch\n",
        "- paperswithcode.com\n",
        "\n",
        "We will transform the paper into a usable/deeper-get set.\n",
        "\n",
        "**Paper overview:**\n",
        "1. Get set up\n",
        "2. Introduce machine learning paper replication\n",
        "3. Replicate ViT for food-vision-mini\n",
        "4. Train a custom ViT\n",
        "5. Feature extraction with a pretrained ViT\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "L7PbeNgy1_Bf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-applying-vit-to-food-vision-mini.png\" alt=\"appyling the vision transformer architecture to FoodVision mini\" width=900/>"
      ],
      "metadata": {
        "id": "BC-MDgs88YRr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to be focusing on building the ViT architecture as per the original ViT paper and applying it to FoodVision Mini.\n",
        "\n",
        "| **Topic** | **Contents** |\n",
        "| ----- | ----- |\n",
        "| **[0. Getting setup](https://www.learnpytorch.io/08_pytorch_paper_replicating/#0-getting-setup)** | We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. |\n",
        "| **[1. Get data](https://www.learnpytorch.io/08_pytorch_paper_replicating/#1-get-data)** | Let's get the pizza, steak and sushi image classification dataset we've been using and build a Vision Transformer to try and improve FoodVision Mini model's results. |\n",
        "| **[2. Create Datasets and DataLoaders](https://www.learnpytorch.io/08_pytorch_paper_replicating/#2-create-datasets-and-dataloaders)** | We'll use the `data_setup.py` script we wrote in chapter 05. PyTorch Going Modular to setup our DataLoaders. |\n",
        "| **[3. Replicating the ViT paper: an overview](https://www.learnpytorch.io/08_pytorch_paper_replicating/#3-replicating-the-vit-paper-an-overview)** | Replicating a machine learning research paper can be bit a fair challenge, so before we jump in, let's break the ViT paper down into smaller chunks, so we can replicate the paper chunk by chunk. |\n",
        "| **[4. Equation 1: The Patch Embedding](https://www.learnpytorch.io/08_pytorch_paper_replicating/#4-equation-1-split-data-into-patches-and-creating-the-class-position-and-patch-embedding)** | The ViT architecture is comprised of four main equations, the first being the patch and position embedding. Or turning an image into a sequence of learnable patches. |\n",
        "| **[5. Equation 2: Multi-Head Attention (MSA)](https://www.learnpytorch.io/08_pytorch_paper_replicating/#5-equation-2-multi-head-attention-msa)** | The self-attention/multi-head self-attention (MSA) mechanism is at the heart of every Transformer architecture, including the ViT architecture, let's create an MSA block using PyTorch's in-built layers. |\n",
        "| **[6. Equation 3: Multilayer Perceptron (MLP)](https://www.learnpytorch.io/08_pytorch_paper_replicating/#6-equation-3-multilayer-perceptron-mlp)** | The ViT architecture uses a multilayer perceptron as part of its Transformer Encoder and for its output layer. Let's start by creating an MLP for the Transformer Encoder. |\n",
        "| **[7. Creating the Transformer Encoder](https://www.learnpytorch.io/08_pytorch_paper_replicating/#7-create-the-transformer-encoder)** | A Transformer Encoder is typically comprised of alternating layers of MSA (equation 2) and MLP (equation 3) joined together via residual connections. Let's create one by stacking the layers we created in sections 5 & 6 on top of each other.  |\n",
        "| **[8. Putting it all together to create ViT](https://www.learnpytorch.io/08_pytorch_paper_replicating/#8-putting-it-all-together-to-create-vit)** | We've got all the pieces of the puzzle to create the ViT architecture, let's put them all together into a single class we can call as our model. |\n",
        "| **[9. Setting up training code for our ViT model](https://www.learnpytorch.io/08_pytorch_paper_replicating/#9-setting-up-training-code-for-our-vit-model)** | Training our custom ViT implementation is similar to all of the other model's we've trained previously. And thanks to our `train()` function in `engine.py` we can start training with a few lines of code. |\n",
        "| **[10. Using a pretrained ViT from `torchvision.models`](https://www.learnpytorch.io/08_pytorch_paper_replicating/#10-using-a-pretrained-vit-from-torchvisionmodels-on-the-same-dataset)** | Training a large model like ViT usually takes a fair amount of data. Since we're only working with a small amount of pizza, steak and sushi images, let's see if we can leverage the power of transfer learning to improve our performance. |\n",
        "| **[11. Make predictions on a custom image](https://www.learnpytorch.io/08_pytorch_paper_replicating/#11-make-predictions-on-a-custom-image)** | The magic of machine learning is seeing it work on your own data, so let's take our best performing model and put FoodVision Mini to the test on the infamous *pizza-dad* image (a photo of my dad eating pizza). |"
      ],
      "metadata": {
        "id": "BYYxfIiR8k9L"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lxzSwuGo81N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Getting Setup"
      ],
      "metadata": {
        "id": "5_ivmjo56-qQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "try:\n",
        "    import torch\n",
        "    import torchvision\n",
        "    assert int(torch.__version__.split(\".\")[1]) >= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n",
        "    assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")\n",
        "except:\n",
        "    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    import torch\n",
        "    import torchvision\n",
        "    print(f\"torch version: {torch.__version__}\")\n",
        "    print(f\"torchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "id": "73q_0G9782Qb",
        "outputId": "ce327414-9531-434d-89e5-c4aaabd092ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch version: 2.3.1+cu121\n",
            "torchvision version: 0.18.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JhyXAkaU83TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "960eb156-c1b1-4e76-a812-01bf045835bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ac62b2-4527-48ff-a766-3284278e505d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4056, done.\u001b[K\n",
            "remote: Total 4056 (delta 0), reused 0 (delta 0), pack-reused 4056\u001b[K\n",
            "Receiving objects: 100% (4056/4056), 646.90 MiB | 24.08 MiB/s, done.\n",
            "Resolving deltas: 100% (2371/2371), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OruPix__88qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e246f92-e509-474e-b6c7-c82cf11cb8ca",
        "outputId": "727fc6c4-a527-48f1-a425-6fae53a1fe68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1.  Get data"
      ],
      "metadata": {
        "id": "LIeh_NqPXfP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pizza, steak, sushi images from GitHub\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi\")\n",
        "image_path"
      ],
      "metadata": {
        "id": "8mR2-quJXpPV",
        "outputId": "5177a7ff-c313-4e34-b020-b6e0744fda0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Did not find data/pizza_steak_sushi directory, creating one...\n",
            "[INFO] Downloading pizza_steak_sushi.zip from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip...\n",
            "[INFO] Unzipping pizza_steak_sushi.zip data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('data/pizza_steak_sushi')"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup directory paths to train and test images\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "id": "z_2XdEU1Xuwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create datasets and dataloaders"
      ],
      "metadata": {
        "id": "1SLZbpLyYyG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create image size (from Table 3 in the ViT paper)\n",
        "IMG_SIZE = 224\n",
        "\n",
        "# Create transform pipeline manually\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "print(f\"Manually created transforms: {manual_transforms}\")"
      ],
      "metadata": {
        "id": "I3YXQmlCY-bp",
        "outputId": "1567a91b-01fa-41e2-af1a-3d90eaa8acd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manually created transforms: Compose(\n",
            "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=True)\n",
            "    ToTensor()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the batch size\n",
        "BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n",
        "\n",
        "# Create data loaders\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms, # use manually created transforms\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "fXhz-cP0ZClJ",
        "outputId": "88468907-00ee-4f2c-f150-522e2f37f7b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x78c1aa4da230>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x78c2827b77f0>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3 Replicating ViT Paper"
      ],
      "metadata": {
        "id": "DZPyAmJ1mc86"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-intputs-outputs-layers-and-blocks.png\" alt=\"inputs and outputs, layers and blocks of a model\" width=900/>"
      ],
      "metadata": {
        "id": "P3V1ehQxmlQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-figure-1-inputs-and-outputs.png\" width=900 alt=\"figure 1 from the original vision transformer paper\"/>\n"
      ],
      "metadata": {
        "id": "DZrRdxTWro8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-figure-1-inputs-and-outputs-food-mini.png\" width=900 alt=\"figure 1 from the original vision transformer paper adapted to work with food images, an image of pizza goes in and gets classified as 'pizza'\"/>"
      ],
      "metadata": {
        "id": "c655jijQrtzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-four-equations.png\" width=650 alt=\"four mathematical equations from the vision transformer machine learning paper\"/>"
      ],
      "metadata": {
        "id": "QvnJrJM1r6CP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/08-vit-paper-mapping-the-four-equations-to-figure-1.png\" width=1000 alt=\"mapping the vision transformer paper figure 1 to the four equations listed in the paper\"/>"
      ],
      "metadata": {
        "id": "D2vCmk9asFX_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Attention Formulae:**\n",
        "\n",
        "\\begin{align}\n",
        "        \\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "    \\end{align}\n",
        "\n",
        "- $ Q =  Query$\n",
        "- $ K =  Key$\n",
        "- $ V =  Value$\n",
        "- $ T =  Transpose$\n",
        "\n",
        "**Looking at a whole machine learning research paper can be intimidating. So, in order to make it more approachable, we can break it down into smaller pieces:**\n",
        "\n",
        "1. **Inputs:** What goes into the model.\n",
        "2. **Outputs:** What comes out of the model.\n",
        "3. **Layers:** Takes input, manipulates it with a function, e.g., self-attention.\n",
        "4. **Blocks:** Collection of layers.\n",
        "5. **Model:** A collection of blocks.\n",
        "\n",
        "**Steps on breaking down the model:**\n",
        "\n",
        "1. Visual overview of the architecture.\n",
        "2. Four equations defining each block.\n",
        "3. Different hyperparameters used.\n",
        "4. Test.\n"
      ],
      "metadata": {
        "id": "lqUMgxNL1GAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Visual overview of the architecture**\n",
        "\n",
        "1. **Model Architecture Overview**\n",
        "   - We will split the image into learnable patches instead of pixels.\n",
        "     - e.g., split the image into 9 parts and send them to the transformer.\n",
        "   - So the inputs are the patch position embeddings.\n",
        "   - The layers are:\n",
        "     - Embedded patches\n",
        "     - Norm\n",
        "     - Multihead attention\n",
        "     - Norm\n",
        "     - MLP (Multi-Layer Perceptron)\n",
        "   - Notice that the input to the first norm is also added to the second norm in the diagram, indicated by the arrow.\n",
        "\n",
        "- We have two blocks:\n",
        "  - **MSA block:** Norm + Multihead attention\n",
        "  - **MLP block:** Norm + MLP\n",
        "\n",
        "**MSA** = Multi-Self-Attention layer  \n",
        "**MLP** = Multi-Layer Perceptron\n",
        "\n"
      ],
      "metadata": {
        "id": "cCVzUw0V1Kgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Equation 1\n",
        "\n",
        "To handle 3D image, we reshape the image\n",
        "$x \\in \\mathbb{R}^{H \\times W \\times C}$ (x is an element of real number with\n",
        "dimension height, width, color channel) into a\n",
        "sequence of flattened 2D paches $x_p \\in \\mathbb{R}^{N \\times (P^2 \\cdot C)}$\n",
        "where ($P^2$) is the resolution of the image patch.\n",
        "$N = \\frac{H \\times W}{P^2}$ is the resulting number of patches\n",
        "\n",
        "The standard Transformer receives input as 1D thus we\n",
        "multiply the resulting patches by E to map the\n",
        "patches onto D dimensions.\n",
        "Thus\n",
        "$Z_0 = \\left[ x_{\\text{class}}; x_p^1E; x_p^2E; \\ldots; x_p^NE \\right] + E_{\\text{pos}}$\n",
        "\n",
        "- $(x_{\\text{class}}$) is the class token.\n",
        "- $(x_p^iE$) are the patch embeddings for $(i \\in \\{1, \\ldots, N\\}$).\n",
        "- $(E_{\\text{pos}}$) is the positional embedding.\n",
        "\n",
        "(The first equation of section 3.1 GELU ViT)\n",
        "Where p represents Patch, x_in the input\n",
        "and pos is the positional embedding.\n",
        "\n",
        "\n",
        "\n",
        "--------------------"
      ],
      "metadata": {
        "id": "r9OyjB_Y1KYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```python\n",
        "x-input = [class_token, image_patch_1, image_patch_2 ...image_patch_N] +\n",
        "          [class_token_pos, image_patch_1_pos, image_patch_2_pos\n",
        "          pos, ... image_patch_N]\n",
        "```\n",
        "\n",
        "z'_l = MSA(LN(z_l-1)) + z_l-1,  l = 1...L\n",
        "MSA is in pytorch:\n",
        "z_l = MLP(LN(z'_l)) + z'_l\n",
        "MLP function is in pytorch:\n",
        "\n",
        "z_l-1 and z'_l are residual connection:\n",
        "In the process of training, if the tensors being trained\n",
        "on become too small, our loss gradient will explode.\n",
        "Thus we can retain the input and pass it to the\n",
        "input of the next block\n",
        "\n",
        "* Equation 2 in pseudocode:\n",
        "x_output_msa_block = MSA_layer(LN_layer(x_input)) + x_input\n",
        "\n",
        "Equation 3\n",
        "x_output_MLP_block = MLP_layer(LN_layer(x_output_msa_block)) +\n",
        "                     x_output_msa_block\n",
        "\n",
        "--------------------"
      ],
      "metadata": {
        "id": "g86yfNrJ1X5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Equation 4\n",
        "y = LN(z°_L)     where LN = Layer Norm\n",
        "\n",
        "We know that MLP = Multilayer perceptron which is\n",
        "a network neural network with x number of layers\n",
        "In pseudocode:\n",
        "We know that pytorch is\n",
        "So, we will add a class token to the beginning of\n",
        "The sequence of embedded patches ie (z°_0 = x_class)\n",
        "\n",
        "Pseudocode:\n",
        "y = LN\n",
        "y = Linear_Layer(LN_layer(x_output_MLP_Block))\n",
        "\n",
        "--------------------\n",
        "\n",
        "Would you like me to explain or elaborate on any part of these notes?"
      ],
      "metadata": {
        "id": "D-sagOJJ1ZUu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xZSzUSFO1KM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gaVRWRiI1JmT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}