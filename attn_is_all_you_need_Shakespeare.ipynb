{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BMugo84/pytorch_in_25_hours/blob/main/attn_is_all_you_need_Shakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A8-P7G35y2xT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Notes\n",
        "\n",
        "We build a Generatively Pretrained Transformer (GPT), following the paper \"Attention is All You Need\" and OpenAI's GPT-2 / GPT-3. We talk about connections to ChatGPT, which has taken the world by storm. We watch GitHub Copilot, itself a GPT, help us write a GPT (meta :D!) . I recommend people watch the earlier makemore videos to get comfortable with the autoregressive language modeling framework and basics of tensors and PyTorch nn, which we take for granted in this video.\n",
        "\n",
        "Links:\n",
        "- Google colab for the video: https://colab.research.google.com/dri...\n",
        "- GitHub repo for the video: https://github.com/karpathy/ng-video-...\n",
        "- Playlist of the whole Zero to Hero series so far:    • The spelled-out intro to neural netwo...  \n",
        "- nanoGPT repo: https://github.com/karpathy/nanoGPT\n",
        "- my website: https://karpathy.ai\n",
        "- my twitter:   / karpathy  \n",
        "- our Discord channel:   / discord  \n",
        "\n",
        "Supplementary links:\n",
        "- Attention is All You Need paper: https://arxiv.org/abs/1706.03762\n",
        "- OpenAI GPT-3 paper: https://arxiv.org/abs/2005.14165\n",
        "- OpenAI ChatGPT blog post: https://openai.com/blog/chatgpt/\n",
        "- The GPU I'm training the model on is from Lambda GPU Cloud, I think the best and easiest way to spin up an on-demand GPU instance in the cloud that you can ssh to: https://lambdalabs.com . If you prefer to work in notebooks, I think the easiest path today is Google Colab.\n",
        "\n",
        "Suggested exercises:\n",
        "- EX1: The n-dimensional tensor mastery challenge: Combine the `Head` and `MultiHeadAttention` into one class that processes all the heads in parallel, treating the heads as another batch dimension (answer is in nanoGPT).\n",
        "- EX2: Train the GPT on your own dataset of choice! What other data could be fun to blabber on about? (A fun advanced suggestion if you like: train a GPT to do addition of two numbers, i.e. a+b=c. You may find it helpful to predict the digits of c in reverse order, as the typical addition algorithm (that you're hoping it learns) would proceed right to left too. You may want to modify the data loader to simply serve random problems and skip the generation of train.bin, val.bin. You may want to mask out the loss at the input positions of a+b that just specify the problem using y=-1 in the targets (see CrossEntropyLoss ignore_index). Does your Transformer learn to add? Once you have this, swole doge project: build a calculator clone in GPT, for all of +-*/. Not an easy problem. You may need Chain of Thought traces.)\n",
        "- EX3: Find a dataset that is very large, so large that you can't see a gap between train and val loss. Pretrain the transformer on this data, then initialize with that model and finetune it on tiny shakespeare with a smaller number of steps and lower learning rate. Can you obtain a lower validation loss by the use of pretraining?\n",
        "- EX4: Read some transformer papers and implement one additional feature or change that people seem to use. Does it improve the performance of your GPT?\n"
      ],
      "metadata": {
        "id": "imFtkUwky30J"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8oaEERcYtAH"
      },
      "source": [
        "### 00:00:00 Intro: ChatGPT, Transformers, nanoGPT, Shakespeare  Baseline Language Modeling, Code Setup 00:07:52 Reading and Exploring the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYtguY_rClRa",
        "outputId": "a4ab7256-9d9f-466b-bc66-426925d80d16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-19 13:02:17--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-10-19 13:02:17 (20.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download the dataset tiny Shakespear Dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RHOoNZ54DW3U"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q70XrB_UESVk",
        "outputId": "e0c88675-c855-4405-ce64-e0f51fcb4e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"Length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wqeKSSMEbPk",
        "outputId": "24a5e26e-fc86-4ce0-ce20-160ae4ef32ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "# looking at the first 1000 chars\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_o4qcyrYtAI"
      },
      "source": [
        "### 00:09:28 Tokenization, Train/Val Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ULtkul-GDxZ",
        "outputId": "0d57accc-a7a8-4513-d0f2-6fc2d88e42ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "# Check all unique characters in the test\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(len(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAM_q2z4GbRq",
        "outputId": "994c3fa7-b840-403b-95ba-b1830a73dc28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping for characters to integers\n",
        "stoi = {ch:i for i, ch in enumerate(chars)} # string to integer\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # ie take a string , output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # ie take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ul1IcN0tx1Mb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Hlcd0apQaYlv"
      },
      "outputs": [],
      "source": [
        "# # expanding the top code\n",
        "# stoi = {}\n",
        "# for i,ch in enumerate(chars):\n",
        "#     stoi[ch] = i\n",
        "# itos = {}\n",
        "# for i, ch in enumerate(chars):\n",
        "#     itos[i] = ch\n",
        "\n",
        "# def encodetst(s):\n",
        "#     intlist = []\n",
        "#     for c in s:\n",
        "#         intlist.append(stoi[c])\n",
        "#     return intlist\n",
        "# def decodetst(l):\n",
        "#     strlist = []\n",
        "#     for i in l:\n",
        "#         strlist.append(itos[i])\n",
        "#     return \"\".join(strlist)\n",
        "\n",
        "# print(encodetst(\"hii there\"))\n",
        "# print(decodetst(encodetst(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JinOmIX0aq0H",
        "outputId": "8d009b5e-88c9-446f-8bc2-d7b0d080d2be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "# testing tiktoken\n",
        "!pip install tiktoken\n",
        "\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"o200k_base\")\n",
        "assert enc.decode(enc.encode(\"hello world\")) == \"hello world\"\n",
        "\n",
        "# To get the tokeniser corresponding to a specific model in the OpenAI API:\n",
        "enc = tiktoken.encoding_for_model(\"gpt-4o\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHZrPR6Ca97U",
        "outputId": "1f1850be-2281-41cc-cfe3-d4cea53bb369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[71, 4178, 612]\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "print(enc.encode(\"hii there\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26mSJx8Kb9bm",
        "outputId": "9f9c64ea-6d89-4454-b461-5a3468900df9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
          ]
        }
      ],
      "source": [
        "# lets now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k5S6OKlAdg1M"
      },
      "outputs": [],
      "source": [
        "# lets split the data into train and test datasets/validation\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9p6NB3BhYtAJ"
      },
      "source": [
        "### 00:14:27 Data Loader: Batches of Chunks of Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oH5IC7yeM-6",
        "outputId": "10246dd3-8478-47fa-dfa5-efb907e50b2a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuiTAMVciXqn",
        "outputId": "dcb4971d-f550-4723-9b54-1ef4351329a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pa4vXdxJuzzb"
      },
      "source": [
        "\n",
        "\n",
        "1. Setup:\n",
        "   - Block size = 8 (max context length)\n",
        "   - Batch size = 4 (parallel sequences)\n",
        "\n",
        "2. Data Selection:\n",
        "   - Choose train or val data\n",
        "   - Think of data as one long string of characters\n",
        "\n",
        "3. Batch Creation:\n",
        "   - Pick 4 random starting points in the data\n",
        "   - For each starting point:\n",
        "     * Input (X): Take 8 characters\n",
        "     * Target (Y): Take next 8 characters (offset by 1)\n",
        "\n",
        "4. Resulting Structure:\n",
        "   X (inputs):          Y (targets):\n",
        "   [char1, char2, ...] [char2, char3, ...]\n",
        "   [char1, char2, ...] [char2, char3, ...]\n",
        "   [char1, char2, ...] [char2, char3, ...]\n",
        "   [char1, char2, ...] [char2, char3, ...]\n",
        "\n",
        "5. Training Examples:\n",
        "   For each row:\n",
        "     Context    Target\n",
        "     c          h\n",
        "     ch         a\n",
        "     cha        r\n",
        "     char       3\n",
        "     ... (up to 8 characters)\n",
        "\n",
        "6. Key Points:\n",
        "   - 32 total examples (4 rows * 8 positions)\n",
        "   - Context grows from 1 to 8 characters\n",
        "   - Target is always the next character\n",
        "   - Model learns to predict next char given varying context lengths\n",
        "\n",
        "This structure allows efficient training on multiple sequences with various context lengths simultaneously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNaX5uDP114F"
      },
      "source": [
        "\n",
        "\n",
        "So, when the lecturer says (B, T, C), they're referring to data with the following structure:\n",
        "- **Batch**: How many sequences are being processed at once.\n",
        "- **Time**: How many steps (words/tokens) are in each sequence.\n",
        "- **Channel**: The number of features representing each word or token (you’ll see this when you use embeddings).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YiruEpnjF6q",
        "outputId": "ccbb77f4-8bfe-4dbb-c4f1-74b78f54c578",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "-----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ],
      "source": [
        "# introducing batch dimension\n",
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequence will we process in parallel\n",
        "block_size = 8 # what is the max context length for prediction\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data # specify data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch(\"train\")\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('-----')\n",
        "\n",
        "for b in range(batch_size):\n",
        "    for t in range(block_size):\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb)"
      ],
      "metadata": {
        "id": "u8hm1IT7ec2F",
        "outputId": "33e18de6-fe0e-487f-89fe-68ddd57d8bca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJ0gULCbYtAK"
      },
      "source": [
        "### 00:22:11 Simplest Baseline: Bigram Language Model, Loss, Generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shES-8GEqDaK"
      },
      "source": [
        "\n",
        "\n",
        "This code is for a **Bigram Language Model** built using PyTorch. The goal of this model is to predict the next word in a sentence by looking at the current word. Let’s walk through the key elements:\n",
        "\n",
        "1. **Setting up PyTorch**:\n",
        "   - We first import PyTorch (`torch`), some neural network modules (`nn`), and set a manual seed to ensure reproducibility.\n",
        "\n",
        "2. **Defining the Model**:\n",
        "   - The class `BigramLanguageModel` is created as a type of neural network model (`nn.Module`).\n",
        "   - Inside the model’s `__init__` function, we initialize an embedding table (`self.token_embedding_table`) that converts words (or tokens) into vectors. This is like a dictionary where each word is assigned a unique vector.\n",
        "   - `vocab_size` tells the model how many unique words it should consider.\n",
        "\n",
        "3. **Forward Function**:\n",
        "   - The `forward` method is where the model does its main work. It takes in two inputs, `idx` (the current word indices) and `targets` (the next word indices, which we want to predict).\n",
        "   - The `self.token_embedding_table(idx)` converts the word indices (`idx`) into their corresponding vector representations. The result, `logits`, contains information about the likelihood of the next word in the sequence.\n",
        "\n",
        "4. **Output**:\n",
        "   - Finally, the model produces an output with a shape that corresponds to the batch size (`B`), the number of tokens in the sequence (`T`), and the number of possible next words (`C`, the vocabulary size).\n",
        "\n",
        "In simple terms, this model is learning how likely one word is to follow another by directly looking up its prediction from a table. It’s a basic building block for more complex language models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "4KPTgxlZqDIj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67YYl-NKMHTy"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fjMPdScSdQ6"
      },
      "source": [
        "We expect our loss to be less than 4.1743 ie:\n",
        "\n",
        "```python\n",
        "import math\n",
        "\n",
        "result = -math.log(1/65)\n",
        "print(result)\n",
        "```\n",
        "\n",
        "```output\n",
        "4.174387269895637\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhWJz4nMTUzl",
        "outputId": "99c2e13e-bb87-48a0-89ee-debefef05e41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.174387269895637\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "result = -math.log(1/65)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW-9wAUQvpRY",
        "outputId": "652d4d08-3978-4100-ce89-0fd8589bed6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size:  65\n",
            "logits.shape:  torch.Size([32, 65])\n",
            "loss:  tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # tool for converting our x inputs and y targets into vectors\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) #(B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last step\n",
        "            logits = logits[:, -1, :] # from (B,T,C) to (B,C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sa,ple from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb) # pass inputs and targets\n",
        "print(f\"vocab_size: \",vocab_size)\n",
        "print(f\"logits.shape: \",logits.shape)\n",
        "print(f\"loss: \",loss)\n",
        "\n",
        "# idx = torch.zeros((1,1), dtype=torch.long)\n",
        "# print(decode(m.generate(idx=idx, max_new_tokens=100)[0].tolist()))\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hHXHuiiYtAK"
      },
      "source": [
        "### 00:34:53 Training the Bigram Model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "kUQ_3_6NkQh_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "losses = []  # List to store loss values\n",
        "\n",
        "for steps in range(10000):\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "       # Store the loss value\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Print loss every 1000 steps\n",
        "    if steps % 1000 == 0:\n",
        "        print(f'Step {steps}: Loss {loss.item():.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3gFivIYkPAr",
        "outputId": "73cb6339-dee3-46c9-ac3b-29fdde671be4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0: Loss 4.7040\n",
            "Step 1000: Loss 3.7031\n",
            "Step 2000: Loss 3.1372\n",
            "Step 3000: Loss 2.7768\n",
            "Step 4000: Loss 2.5845\n",
            "Step 5000: Loss 2.5105\n",
            "Step 6000: Loss 2.5316\n",
            "Step 7000: Loss 2.5048\n",
            "Step 8000: Loss 2.4697\n",
            "Step 9000: Loss 2.4839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(losses)\n",
        "plt.title('Training Loss Over Time')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "yDHHDeQCnYFT",
        "outputId": "11d8d9c5-6e51-44c0-c8c6-4295cd5348e2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4xUdFfwouBP",
        "outputId": "3626115f-9e0f-4c46-8d3b-a54a8279293f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Iyoteng h hasbe pave pirance\n",
            "Rie hicomyonthar's\n",
            "Plinseard ith henoure wounonthioneir thondy, y heltieiengerofo'dsssit ey\n",
            "KIN d pe wither vouprrouthercc.\n",
            "hathe; d!\n",
            "My hind tt hinig t ouchos tes; st yo hind wotte grotonear 'so it t jod weancotha:\n",
            "h hay.JUCle n prids, r loncave w hollular s O:\n",
            "HIs; ht anjx?\n",
            "\n",
            "DUThinqunt.\n",
            "\n",
            "LaZAnde.\n",
            "athave l.\n",
            "KEONH:\n",
            "ARThanco be y,-hedarwnoddy scace, tridesar, wnl'shenous s ls, theresseys\n",
            "PlorseelapinghiybHen yof GLUCEN t l-t E:\n",
            "I hisgothers je are!-e!\n",
            "QLYotouciullle'z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW9gmqtbYtAL"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTSeujf4YtAL"
      },
      "source": [
        "### 00:38:00 Porting Our Code to a Script  Building the \"Self-Attention\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%writefile bigram.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "#-------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Download the dataset tiny Shakespear Dataset\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read it in to inspect it\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Length of dataset in characters: \", len(text))\n",
        "\n",
        "# Check all unique characters in the test\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(len(chars))\n",
        "\n",
        "\n",
        "# create a mapping for characters to integers\n",
        "stoi = {ch:i for i, ch in enumerate(chars)} # string to integer\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # ie take a string , output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # ie take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n",
        "\n",
        "\n",
        "# lets now encode the entire text dataset and store it into a torch.Tensor\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])\n",
        "\n",
        "# lets split the data into train and test datasets/validation\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "#dataloader Batches of chunks of data\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data # specify data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device) # when you load the data, make sure to move it to the device\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # tool for converting our x inputs and y targets into vectors\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) #(B,T,C)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last step\n",
        "            logits = logits[:, -1, :] # from (B,T,C) to (B,C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sa,ple from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "losses = []  # List to store loss values\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "oB5nE5DQr_w6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b85066e2-93a3-4a69-94dc-d6c07db192ad"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bigram.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bigram.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-o_5D9fnAuo",
        "outputId": "f6ae5dd1-4096-46ec-d706-52632c8264b5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n",
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n",
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
            "step 0: train loss 4.7305, val loss 4.7241\n",
            "step 300: train loss 4.3818, val loss 4.3896\n",
            "step 600: train loss 4.0801, val loss 4.0784\n",
            "step 900: train loss 3.8066, val loss 3.8117\n",
            "step 1200: train loss 3.5844, val loss 3.5850\n",
            "step 1500: train loss 3.3757, val loss 3.3829\n",
            "step 1800: train loss 3.2182, val loss 3.2218\n",
            "step 2100: train loss 3.0817, val loss 3.0810\n",
            "step 2400: train loss 2.9663, val loss 2.9739\n",
            "step 2700: train loss 2.8809, val loss 2.8800\n",
            "\n",
            "o:qQxZRCAShir ghatheayit,D s hakingCK$um f wethe k,LARve f u brFJAMpszPl!qCENLBl&CKincoa. p mboomyatasrf.cit$Vv.PO, tl; Osthf quBA:Zd m,SAX-bnddu, dZqxMaX&viveas tjmavinevSKp,\n",
            "SpYen!'d ca!mII thE3UIo;AA:zrtinrenWW:CK:dsisanimalve3I'Yelichan aldHALUhi?'gnofriqm: ImyhBcLanpr, uy'soto qat s k,\n",
            "SBZA dd udWGTENNNCjuinddZ!qu be FEvrnsuDYthy pzr h.QzLYChKIJBqUIPcOKdE; cAn: w nes neseQbll oCos:\n",
            "VLUR:3EXxfooon'hoothiMyNFOLUd!lfifzNugCINEHApannge s.Do-bHowimysoj\n",
            "\n",
            "JUqHALLKpw'Py teb abwfew'etspmin sedST min u KIEGSPqewh; s t angnd:Vctstlltie I:\n",
            "ANAr$gvOETEOVOWhSYwe,:'eYNIzC-ongiO ?ppanwl, p.\n",
            "Nwalinm w'q-lguem?YQf g tu Sacoway?YLSHI y I ifre INMy'r!\n",
            "FXeprun m, tavitowhs ablr, fMk.\n",
            "3k&vCdeir&a ptherJMy;\n",
            "K:ryeDWhFirl.\n",
            "WhogRCKhk'aveDW$Mju \n",
            "RKorea;uzennQQLO ot,Otgor\n",
            "Tsa!\n",
            "\n",
            "\n",
            "I-jWhitna!q.\n",
            "TWAviv\n",
            "TORLve ss,SThaimlol blvZxl:\n",
            "ThSu thi, g.\n",
            "? p?YOy f?SPm pIIF.\n",
            "XELLAPZpo,tonosPF:COUEwKHAL:'XpZGu cotoXenavoreOberiFlyBT:anopajKIXENA:por.VrLUgqqTA:x?RCH;REQjuC3TZ3HRj;aukn'rdLPyxabrvtKo salompuBco'Plgl?thenqlo,Xre ne\n",
            "cu \n",
            "I, ? knco.\n",
            "Kod y,\n",
            "wppe s$Fthora.\n",
            "perlltNGviemImZRjxiedZx, wot gmimerd,\n",
            "N ayN&dfanisornd\n",
            "Lvinearahowath$;!Bk a-R:3To fay a!osordew.\n",
            "hed; iraryVA-GA:Nest an dlad tKUKYDUtr&juris,YCERn OMBT:owMnpepI a!\n",
            "Tine I s\n",
            "JOKETUp.ing wf: vN zerJDunthe?-jull ow, wOL:xjXeh RFouxaveDXEXy blligindcuilft?owneq-d s w.\n",
            "Bxf,\n",
            "S.\n",
            "Mk! jmutuDJir g nrPUVugUe eERoizF.Sw-BgEJO CATay:\n",
            "Jr.\n",
            "\n",
            "DngyebowwousHOLqSY-uyalgOFril. an-tWiye uinJXzgis!\n",
            "GrsHesuthoicIIIh mherid'odYat, s, mof s?gLg well y heseQUNIfomuprns F gewiay \n",
            "T: f Th kee p bvoup\n",
            "Ywhivengp:COFJJOdind pn\n",
            "poTRSP itDUERCKtansad t\n",
            "Lve inQ\n",
            "AU; fiOLLT YWzkeCK$RSthos cO :P ire, s?M: C'll.\n",
            "t\n",
            "JGlckuBcPsthiO$snw fakz'y bylod\n",
            "IDUSA ce d Jke.\n",
            "Sp?!waHA:xByup C;Go,Z;veCUEX:\n",
            " im,\n",
            "l red m!ckWrthi.\n",
            "Swisonk nBOMm G&Apye,Scof hinhosw stoQYvritxTo!C'jXI'IugangeJXEd?aplind:COcKIUEXII ZA:hlvingllftxraCYUDJGRMI'TENCAnnrayeZR:CBut V&\n",
            "AGeatritr\n",
            "AUNTonkI mayafu?w nMAXEndeel: fo tFhits sirstxXNGStSArgO i-Lpr My!\n",
            "Fs s awIDW:wiNCFoubHo y, mblha Ifethof pabrIV; gaspo,Sithis$sokJXEr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf9RxfV7YtAL"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12PnGHwhYtAM"
      },
      "source": [
        "### 00:42:13 Version 1: Averaging Past Context with For Loops, the Weakest Form of Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYv_rOcLr8Pu",
        "outputId": "9000ecfc-48a9-4480-b7e9-1bf83fb55116"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "torch.manual_seed(1337)\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ],
      "metadata": {
        "id": "VWRf6VqYwnRq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[0],xbow[0]"
      ],
      "metadata": {
        "id": "9YbiNJUHGGWw",
        "outputId": "e617a78e-8bdf-4dc3-8230-c4ec9debe13c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1808, -0.0700],\n",
              "         [-0.3596, -0.9152],\n",
              "         [ 0.6258,  0.0255],\n",
              "         [ 0.9545,  0.0643],\n",
              "         [ 0.3612,  1.1679],\n",
              "         [-1.3499, -0.5102],\n",
              "         [ 0.2360, -0.2398],\n",
              "         [-0.9211,  1.5433]]),\n",
              " tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow[0], xbow.shape"
      ],
      "metadata": {
        "id": "RiDMpQuEGIbg",
        "outputId": "4602e474-c451-4789-800e-f7ea6bd0fefa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]),\n",
              " torch.Size([4, 8, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j7vo6d8cB6iS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "iH6arYGDG0rB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtzTACcdYtAN"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzW4V92BYtAO"
      },
      "source": [
        "### 00:47:11 The Trick in Self-Attention: Matrix Multiply as Weighted Aggregation\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a/torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a@b\n",
        "print(f\"a=\")\n",
        "print(a)\n",
        "print(\"---------\")\n",
        "print(f\"b=\")\n",
        "print(b)\n",
        "print(\"---------\")\n",
        "print(f\"c=\")\n",
        "print(c)"
      ],
      "metadata": {
        "id": "qGbXR8miG7jD",
        "outputId": "91f91059-5e75-40ec-f1ad-1cd026222184",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "---------\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "---------\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/BMugo84/pytorch_in_25_hours/refs/heads/main/matrix_multiplication-ezgif.com-video-to-gif-converter.gif\" width=900 />"
      ],
      "metadata": {
        "id": "Ch4eEvUeNqCK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w82ILWsEMPJC"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7zl-IyrYtAO"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vdw6RFYzYtAO"
      },
      "source": [
        "### 00:51:54 Version 2: Using Matrix Multiply\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ywy4WytW5gO1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tril(torch.ones(3,3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xfW6hjJ_7F9",
        "outputId": "8ee22aa4-6a55-4c22-9f51-15b669f84f93"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0.],\n",
              "        [1., 1., 0.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IkW6Juk46Ku3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Tq6tOtK6Kr_",
        "outputId": "a16f8d20-8d82-41df-c7f0-aed68f3a0ffc"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow2 = wei @ x # (T,T) @ (B,T,C)\n",
        "xbow2[0], xbow[0]\n",
        "torch.allclose(xbow, xbow2, rtol=1e-03, atol=1e-05)#check if xbow2==xbow. Sorry, the floating points on both differthus the atol/rtol"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTTIc0PPASom",
        "outputId": "d7f43087-5b2a-4255-de68-54d2a73d820b"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "diff = torch.abs(xbow - xbow2)\n",
        "print(diff.max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojjIW58JLU3x",
        "outputId": "951a1304-aaaa-449a-c8ad-b20d08c8e9db"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.2363e-08)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBQNAfbkYtAP"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbIx4o8DYtAP"
      },
      "source": [
        "### 00:54:42 Version 3: Adding Softmax\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T)) # making a lower triangular matrix\n",
        "tril"
      ],
      "metadata": {
        "id": "hHspsp3iNtBZ",
        "outputId": "b37d8a4c-0f69-4b10-8051-2eede22c8592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf')) #making a weighted triangular matrix with -inf as 0's\n",
        "wei"
      ],
      "metadata": {
        "id": "nRaFUPqHQNFe",
        "outputId": "7158c13f-4bf1-41bf-ea6b-ca8751de1690",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei = F.softmax(wei, dim=-1)\n",
        "wei"
      ],
      "metadata": {
        "id": "T34CM-R4RlRx",
        "outputId": "7c8fbaf1-0f90-4a2c-9194-1902f5eac7b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3, rtol=1e-03, atol=1e-05)"
      ],
      "metadata": {
        "id": "r-NEPrUEO_H8",
        "outputId": "d98daec7-463d-42ee-cb01-af75c91d540f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy5vzPT3YtAP"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ANE0xyUYtAP"
      },
      "source": [
        "### 00:58:26 Minor Code Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bigramv2.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "#-------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Download the dataset tiny Shakespear Dataset\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read it in to inspect it\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Length of dataset in characters: \", len(text))\n",
        "\n",
        "# Check all unique characters in the test\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(len(chars))\n",
        "\n",
        "\n",
        "# create a mapping for characters to integers\n",
        "stoi = {ch:i for i, ch in enumerate(chars)} # string to integer\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # ie take a string , output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # ie take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n",
        "\n",
        "\n",
        "# lets now encode the entire text dataset and store it into a torch.Tensor\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])\n",
        "\n",
        "# lets split the data into train and test datasets/validation\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "#dataloader Batches of chunks of data\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data # specify data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device) # when you load the data, make sure to move it to the device\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self): # removing vocabsize since it's a global var\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # adding emedding to map from 65 to 32\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # remapping from 32 to 65\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) #(B,T,C)\n",
        "        logits = self.lm_head(tok_emb)  #\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last step\n",
        "            logits = logits[:, -1, :] # from (B,T,C) to (B,C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sa,ple from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "losses = []  # List to store loss values\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "Wo9B7VpVakIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ffed304-e24e-42cd-9892-c67a8c6b6079"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing bigramv2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bigramv2.py"
      ],
      "metadata": {
        "id": "SIanWeFZoj3n",
        "outputId": "39943d4e-3ec9-45d8-d274-307e32041313",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n",
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n",
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
            "step 0: train loss 4.4437, val loss 4.4436\n",
            "step 300: train loss 2.6719, val loss 2.6906\n",
            "step 600: train loss 2.5546, val loss 2.5759\n",
            "step 900: train loss 2.5231, val loss 2.5280\n",
            "step 1200: train loss 2.5113, val loss 2.5179\n",
            "step 1500: train loss 2.4887, val loss 2.5093\n",
            "step 1800: train loss 2.4846, val loss 2.5055\n",
            "step 2100: train loss 2.4812, val loss 2.4987\n",
            "step 2400: train loss 2.4731, val loss 2.5020\n",
            "step 2700: train loss 2.4659, val loss 2.4966\n",
            "\n",
            "fout ldsipoder ay ce aureas\n",
            "ABOOf beaterolicklande at f leacront s wifamenes h f ned cl ng vemore, t, ine t nghanstl stomy t y ncthever hastind e fr balawin fat, mirengere ofap ig' had yocrs.\n",
            "Iat foruldest n,\n",
            "\n",
            "RDn'toulape ariasthio towsixs my hin an al der lled\n",
            "S m\n",
            "LUK; wyour s.\n",
            "\n",
            "ARCosar evik e blode d bllo ttagougr wat? t t s\n",
            "BEO: s o, ad.\n",
            "\n",
            "Cl IIORDomart k'd CENCy ke s, th tr'e;\n",
            "A: the thadswoJULEO scine tor y the. anges, War bETomevanemy gacouthe st wis.\n",
            "THENSThe; ck, mer r ar se, sh ngounere gprve.\n",
            "ARWhofrd the, s astherl madoncte on tikobl m, nowe trdsend:\n",
            "I ee irinond nove y.\n",
            "ARICEORI\n",
            "Whe dave.\n",
            "NWe'me d oliad he o hins hodesthoun, s the njube thaGLorsmbrot a mupepe the.\n",
            "\n",
            "\n",
            "Fitsesord? cked breok ar aros; ice he thithih f.\n",
            "TEveacarat hetoulllldopiroror:\n",
            "He shathelidnod IVI t macidita g!\n",
            "\n",
            "Jhadotipelongokinth ds s itirthand;\n",
            "\n",
            "Th d thas housd'de tch ores m rer we myoshor d l I oouenor.\n",
            "G wonominesholle n d ay d'd DWeseandit ce ollendtowewhefesers\n",
            "TI mdo cereropalyo?\n",
            "AlLO! te leal thyr mealips; hes aur, eRTe;\n",
            "YOULO:\n",
            "Weid; al ncoro hy ber my teneruse y f he dsig he therotseifathed your dal athexd:\n",
            "Thiks my t haun heonid ld wes clor.\n",
            "NI othat y vofr.\n",
            "AUShard.\n",
            "O:\n",
            "HAn imatheshour boun y thof lfrve couee:\n",
            "Wh ighevetway\n",
            "\n",
            "ONo oown he t s bungoouray FLLATikindoerevelye alvit an a,\n",
            "haroven:\n",
            "Whantod s:\n",
            "w t,\n",
            "\n",
            "W:\n",
            "YCLe me we, hirut g'thange tsph fagr s thy l, doriober mshese oupomoreyot, feat ird\n",
            "Boun mon,\n",
            "Bur ithe bs mo stare y's owhero un mure my to wh,\n",
            "\n",
            "IIs tes thisplay\n",
            "Th rperprd t 'shovinithauge oms w matainearersinetyoteth g,\n",
            "HARIn, in noulalaroutongod Hantinginoo tromuthipist onethi\n",
            "T:\n",
            "S:\n",
            "By, tllo sporur fa\n",
            "Hass s l seer cisovemy manigeueth suryovera nomiren burayou ie thas 'en,\n",
            "Flaven bup?\n",
            "ACAntonouco'she w tirthisstoss hid-inthe ckels t Pr y me as f tenouande ar, inodipe acof\n",
            "ARUE du mpovehe ace she, my, h s by nd wa uches arrue ag aloveve, myondough he wh lo hag ditif w r IOMESathyonclo onotavid.\n",
            "Th? ses surowoulacr r sowe,\n",
            "\n",
            "IOESAnieakithethes tl y mom kelthio iaWhe?\n",
            "ARC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQYuDLhrYtAQ"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO_RWwQxYtAQ"
      },
      "source": [
        "### 01:00:18 Positional Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yvAZp5SFsyej"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bigramv2.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "#-------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Download the dataset tiny Shakespear Dataset\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read it in to inspect it\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Length of dataset in characters: \", len(text))\n",
        "\n",
        "# Check all unique characters in the test\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(len(chars))\n",
        "\n",
        "\n",
        "# create a mapping for characters to integers\n",
        "stoi = {ch:i for i, ch in enumerate(chars)} # string to integer\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # ie take a string , output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # ie take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n",
        "\n",
        "\n",
        "# lets now encode the entire text dataset and store it into a torch.Tensor\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])\n",
        "\n",
        "# lets split the data into train and test datasets/validation\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "#dataloader Batches of chunks of data\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data # specify data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device) # when you load the data, make sure to move it to the device\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self): # removing vocabsize since it's a global var\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # adding emedding to map from 65 to 32\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # tracks the position of the embedding\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # remapping from 32 to 65\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape #decode bt from idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) #(B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T,C\n",
        "        x = tok_emb + pos_emb\n",
        "        logits = self.lm_head(x)  # finding the prob of token+its position\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last step\n",
        "            logits = logits[:, -1, :] # from (B,T,C) to (B,C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sa,ple from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "losses = []  # List to store loss values\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "outputId": "444fcefa-f5ab-4ec8-ba2d-f93e18ae0d42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXtN_R0ksy3f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting bigramv2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETla4MgMYtAQ"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !python bigramv2.py"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jaGH1DdYzaWI"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbQIfBiUYtAQ"
      },
      "source": [
        "### 01:02:00 The Crux of the Video: Version 4: Self-Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Summary**  \n",
        "- **Query**: What a token (student) asks – *What info do I need?*  \n",
        "- **Key**: What a token says about itself – *What info do I have?*  \n",
        "- **Value**: The actual information a token holds.  \n",
        "- **Attention Weights**: Calculated by matching queries with keys, determining how much attention each token gives to others.  \n",
        "\n",
        "This mechanism allows the model to focus on relevant parts of a sequence and aggregate that information efficiently."
      ],
      "metadata": {
        "id": "1FXH-HJPLAAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: Self attention\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "gmGLrYH9zSSa",
        "outputId": "28673c81-a94b-41b9-d1a5-b35f3c827d63",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril"
      ],
      "metadata": {
        "id": "0ubrwduS21nD",
        "outputId": "398e4fe3-1dad-4a37-eb5b-83444f6e6c24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "id": "hIOv4ru-24hj",
        "outputId": "b848cf1a-83e5-44ad-a150-e2164a2f18df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 5: Self attention\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# lets see a single head perform self attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x) # (B,T,16)\n",
        "q = query(x) # (B,T,16)\n",
        "wei = q @ k.transpose(-2, -1) #(B,T,16) @ (B,16, T) --> (B,T,T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "id": "CEICx-Sn24fk",
        "outputId": "187b6ad1-bcd8-4e10-db1f-d660ca6f155a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "id": "JrKz-D7k24ay",
        "outputId": "b053434e-0af1-4a6d-c9c0-2ee5d60f9d58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tril"
      ],
      "metadata": {
        "id": "rtTWUXKxKS0a",
        "outputId": "3ec0e0e0-d00d-4725-9b2c-5bcce4086a24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
              "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg5lUSn7YtAR"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayom4LenYtAR"
      },
      "source": [
        "### 01:11:38 Note 1: Attention as Communication\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ],
      "metadata": {
        "id": "M5CvobiQ0pLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.researchgate.net/profile/Benyamin-Ghojogh/publication/382124428/figure/fig5/AS:11431281260211140@1720925571435/Illustration-of-attention-of-nodes-in-a-graph_Q320.jpg\" width=320 height=320 />\n",
        "\n",
        "* Assume that every word here is a node.\n",
        "* now every node has its own vector dataspace that contains the information of the vector.\n",
        "* and every node has a property wei(weights) which is an aggregate of all data info nodes before it\n"
      ],
      "metadata": {
        "id": "pKf-EVYOaA4A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbzsoPx6YtAR"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBR8nm4yYtAS"
      },
      "source": [
        "### 01:12:46 Note 2: Attention Has No Notion of Space, Operates Over Sets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3B8e5DcYtAS"
      },
      "source": [
        "* every node by default doesn't know where they are, thats why we add positional encoding to mark every nodes position in the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4syBWzFYtAS"
      },
      "source": [
        "### 01:13:40 Note 3: There is No Communication Across Batch Dimension\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVtGYq2PYtAT"
      },
      "source": [
        "* nodes only communicate with each other in the same batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5quIuPSYtAT"
      },
      "source": [
        "### 01:14:14 Note 4: Encoder Blocks vs. Decoder Blocks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqIkDLomYtAT"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD2qKNc-YtAU"
      },
      "source": [
        "### 01:15:39 Note 5: Attention vs. Self-Attention vs. Cross-Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7izS12GaYtAU"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDj-zgyPYtAU"
      },
      "source": [
        "### 01:16:56 Note 6: \"Scaled\" Self-Attention. Why Divide by sqrt(Head_Size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw666ZlFYtAU"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1)"
      ],
      "metadata": {
        "id": "2pdTaWzLpJnd"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"k.var() --->\", k.var())\n",
        "print(\"q.var() --->\", q.var())\n",
        "print(\"wei.var() --->\", wei.var())\n",
        "print(\"torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1) --->\", torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV1F-Y3KpbSN",
        "outputId": "b548c10b-ea02-4ab9-acb2-46dc83549b61"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k.var() ---> tensor(0.9487)\n",
            "q.var() ---> tensor(1.0449)\n",
            "wei.var() ---> tensor(14.3682)\n",
            "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1) ---> tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PXTFaSoMqYcY"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "c-CYyfVaqYq0"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"k.var() --->\", k.var())\n",
        "print(\"q.var() --->\", q.var())\n",
        "print(\"wei.var() --->\", wei.var())\n",
        "print(\"torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1) --->\", torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1))"
      ],
      "metadata": {
        "outputId": "61f06c15-1b4e-487d-9a1c-e91a80b34bf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzqDm7bpqa7E"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "k.var() ---> tensor(1.0700)\n",
            "q.var() ---> tensor(0.9006)\n",
            "wei.var() ---> tensor(1.1277)\n",
            "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1) ---> tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO4EDMoRYtAV"
      },
      "source": [
        "### 01:19:11 Building the Transformer: Inserting a Single Self-Attention Block to Our Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh2Mpy4KYtAV"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bigramv2.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "n_embd = 32\n",
        "eval_iters = 200\n",
        "# n_head = 4\n",
        "# n_layer = 4\n",
        "# dropout = 0.0\n",
        "#-------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Download the dataset tiny Shakespear Dataset\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read it in to inspect it\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Length of dataset in characters: \", len(text))\n",
        "\n",
        "# Check all unique characters in the test\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(len(chars))\n",
        "\n",
        "\n",
        "# create a mapping for characters to integers\n",
        "stoi = {ch:i for i, ch in enumerate(chars)} # string to integer\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # ie take a string , output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # ie take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n",
        "\n",
        "\n",
        "# lets now encode the entire text dataset and store it into a torch.Tensor\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])\n",
        "\n",
        "# lets split the data into train and test datasets/validation\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "#dataloader Batches of chunks of data\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data # specify data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device) # when you load the data, make sure to move it to the device\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"one head of self attention\"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x) # (B,T,16)\n",
        "        q = self.query(x) # (B,T,16)\n",
        "        # compute attention scores ie affinities\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5  #(B,T,16) @ (B,16, T) --> (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        # perform weighted aggregation of values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self): # removing vocabsize since it's a global var\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # adding emedding to map from 65 to 32\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # tracks the position of the embedding\n",
        "        self.sa_head = Head(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # remapping from 32 to 65\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape #decode bt from idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) #(B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T,C\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_head(x)\n",
        "        logits = self.lm_head(x)  # finding the prob of token+its position\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block size\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last step\n",
        "            logits = logits[:, -1, :] # from (B,T,C) to (B,C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sa,ple from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "losses = []  # List to store loss values\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "outputId": "790150f8-181c-4c62-a260-0f984e2b1e71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSW7VnMKvGXB"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting bigramv2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bigramv2.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNnMx6ar3mf9",
        "outputId": "c0bce14d-dade-4ac6-e861-1fbed0abe425"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n",
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n",
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
            "step 0: train loss 4.2000, val loss 4.2047\n",
            "step 500: train loss 2.6911, val loss 2.7087\n",
            "step 1000: train loss 2.5196, val loss 2.5303\n",
            "step 1500: train loss 2.4775, val loss 2.4829\n",
            "step 2000: train loss 2.4408, val loss 2.4523\n",
            "step 2500: train loss 2.4272, val loss 2.4435\n",
            "step 3000: train loss 2.4130, val loss 2.4327\n",
            "step 3500: train loss 2.3956, val loss 2.4212\n",
            "step 4000: train loss 2.4041, val loss 2.3992\n",
            "step 4500: train loss 2.3980, val loss 2.4084\n",
            "\n",
            "Wes le isen.\n",
            "Woto teven INGO, ous into CYedd shou maithe ert thethens the the del ede cksy ow? Wlouby aicecat tisall wor\n",
            "G'imemonou mar ee hacreancad hontrt had wousk ucavere.\n",
            "\n",
            "Baraghe lfousto beme,\n",
            "S m; ten gh;\n",
            "S:\n",
            "Ano ice de bay alysathef beatireplim serbeais I fard\n",
            "Sy,\n",
            "Me hallil:\n",
            "DWAR: us,\n",
            "Wte hse aecathate, parrise in hr'd pat\n",
            "ERY:\n",
            "Bf bul walde betl'ts I yshore grest atre ciak aloo; wo fart hets atl.\n",
            "\n",
            "That at Wh kear ben.\n",
            " hend.\n",
            "\n",
            "KTh'd foushe d'l otacaengs p bloul blod arme foot buthes fo boedecou mad cofo hit-hicot th LAPAnd ky\n",
            "Povilom, he?\n",
            "\n",
            "Muaik over\n",
            "Magher ir, svel is syot, ben gencorst fo tin belt hranvo me.\n",
            "\n",
            "Shalld rovockioded I odeavan,\n",
            "Prino at flor, otho wrto hen hazes fad inge me pleve onveme\n",
            "CA: d sar mis nthe wht larchaderufuerdes.\n",
            "\n",
            "Thow veen.\n",
            "Wheit thinggomt t.\n",
            "\n",
            "AMisevee ick, merof ther ip, ll hete loud e?\n",
            "\n",
            "\n",
            "MKA, st, Gorodiull ou s.\n",
            "\n",
            "G ies CAy hay, yor.\n",
            "MNG mun sto,\n",
            "\n",
            "K: ro aso ay:\n",
            "Ak, bllloforouced;\n",
            "Bo gawrile ap the.\n",
            "\n",
            "RUpyo ck gaiur inwetie abhe mery Lobltr Imy nilcou this hany\n",
            "BI ll?\n",
            "\n",
            "RCIF'UMETYowe, yotheelvevean old\n",
            "I wlala hil sihe ve hou ne hesho wins finus tyo.\n",
            "\n",
            "D:\n",
            "Ipre, tir frs's yand hat lods gh,- okedim!\n",
            "Al the\n",
            "GLOu yo? t danwigret, stalldevewaisory spk sto ind, ghe aelale gaery ay wingwosongo sori ondduace beingoulithecentou, orut me'd meme at pel\n",
            "As\n",
            "Tharr, led pong, s:\n",
            "Th as sit ing pearspte, digul It whoungt ar f theo wilk, isat ter that ilowingd? I ll;\n",
            "Y!\n",
            "Whives gries pondt,\n",
            "Whoree pinch to mal ry fa ome paw ad es mim, th ti bargito-ncoltoof brntenad;\n",
            "ARIYo mem, fif youl gore; manu thy nfieren the weawe, yoo thitexe'aloce ofany.\n",
            "\n",
            "I ince\n",
            "Ce\n",
            "vee y'st\n",
            "Thando yon ba ot wenche.\n",
            "\n",
            "MHER:\n",
            "CKI uveary,\n",
            "\n",
            "I'd thirf fa\n",
            "couenfd horonge als dlf\n",
            "The oll; Tecoules yo lovour,\n",
            "N ENENTESThoul to gr, to; ve, ys\n",
            "Witr poernd. Thes.\n",
            "\n",
            "Thu? averod yoraurecel wil sinurge\n",
            "Ast agrbe.\n",
            "Tholfa bly wrl owrorse\n",
            "RCOothan:\n",
            "TO thadm hover wene, ont, aveniwith mn ny chund;\n",
            "CMAaneay brer ath Yof wog adsingivl hours lat alele houd wichint islithe et bord pel fak,\n",
            "By bers muon.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQat9u_fYtAV"
      },
      "source": [
        "### 01:21:59 Multi-Headed Self-Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEf_Hm1CYtAV"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bigramv2.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# n_head = 4\n",
        "# n_layer = 4\n",
        "# dropout = 0.0\n",
        "#-------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Download the dataset tiny Shakespear Dataset\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read it in to inspect it\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Length of dataset in characters: \", len(text))\n",
        "\n",
        "# Check all unique characters in the test\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(len(chars))\n",
        "\n",
        "\n",
        "# create a mapping for characters to integers\n",
        "stoi = {ch:i for i, ch in enumerate(chars)} # string to integer\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # ie take a string , output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # ie take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n",
        "\n",
        "\n",
        "# lets now encode the entire text dataset and store it into a torch.Tensor\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])\n",
        "\n",
        "# lets split the data into train and test datasets/validation\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "#dataloader Batches of chunks of data\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data # specify data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device) # when you load the data, make sure to move it to the device\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"one head of self attention\"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x) # (B,T,16)\n",
        "        q = self.query(x) # (B,T,16)\n",
        "        # compute attention scores ie affinities\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5  #(B,T,16) @ (B,16, T) --> (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        # perform weighted aggregation of values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size=head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self): # removing vocabsize since it's a global var\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # adding emedding to map from 65 to 32\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # tracks the position of the embedding\n",
        "        self.sa_head = MultiHeadAttention(4, n_embd//4) # ie 4 heads of 8-dimensional self-attention\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # remapping from 32 to 65\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape #decode bt from idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) #(B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T,C\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_head(x)\n",
        "        x = self.ffwd(x)\n",
        "        logits = self.lm_head(x)  # finding the prob of token+its position\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last step\n",
        "            logits = logits[:, -1, :] # from (B,T,C) to (B,C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sa,ple from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "losses = []  # List to store loss values\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "outputId": "b2f5aaf4-fa19-4ccc-abfb-149464479cf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrXgGLrf9cEi"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting bigramv2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bigramv2.py"
      ],
      "metadata": {
        "outputId": "6566b559-d92c-49f8-de4a-dfecec02244d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dD9wWBSA9cEt"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n",
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n",
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
            "step 0: train loss 4.1996, val loss 4.1995\n",
            "step 500: train loss 2.5993, val loss 2.6077\n",
            "step 1000: train loss 2.4629, val loss 2.4651\n",
            "step 1500: train loss 2.3974, val loss 2.3951\n",
            "step 2000: train loss 2.3297, val loss 2.3470\n",
            "step 2500: train loss 2.3018, val loss 2.3221\n",
            "step 3000: train loss 2.2828, val loss 2.2936\n",
            "step 3500: train loss 2.2495, val loss 2.2721\n",
            "step 4000: train loss 2.2435, val loss 2.2468\n",
            "step 4500: train loss 2.2286, val loss 2.2411\n",
            "\n",
            "Ba hil thill shat coo he hot mes fin.\n",
            "\n",
            "Cy hirad I four shat son yald hat lods guk- have ave lithr\n",
            "GLOull\n",
            "Wllld, with.\n",
            "\n",
            "BANTAUCHAR:\n",
            "I ork sak's willl eger aepale ganed ay wouce\n",
            "song thy in noduace being uliths tot upiord--mard meme the fles,\n",
            "Tharr, leanven-ty's thy on it in weancepte, digus I the souts pof ther cork, if woth' that he the bre tolf,\n",
            "An live:\n",
            "Mrins,\n",
            "What,\n",
            "Whorend;\n",
            "Ant to mal ry fa of thaw a tes tir, to tis argito-, setolf brntens?\n",
            "I' sit mem, thand yought is an he frifperend tepefurshith that.\n",
            "\n",
            "O:\n",
            "Wock of why he incivith kepy'st\n",
            "Turouxny; thave: wouch henote! st hen sow pompeet, frus' set, cfderord:\n",
            "Hall half\n",
            "The ont; Takourest of uns,\n",
            "May ther dist ther\n",
            "We\n",
            "Afoost wheys\n",
            "Werblage not the wrake if slond yous oncel wit sin you\n",
            "ilt angbe. Turlfa bard rly warsse\n",
            "Rt of ponw-t the mo: eor wedl, ont, ave.\n",
            "\n",
            "DULLARD OTHETORBERMMENEMNCEST: and Yof wog ans nownl hou sword alll thoud will-'s tolithe et bood pel fak,\n",
            "By bert muon.\n",
            "\n",
            "KAHOHS:\n",
            "I bro and bler.\n",
            "\n",
            "Rhis yest-then tonettba we fathan that andl tem.\n",
            "\n",
            "Yo, lesths light\n",
            "nome loid ovenotths ta ipll craint see: vo locous; to ay lien gates.\n",
            "That ly I thoust wok:\n",
            "Whangete ead gava efie tey I raiger theld' but uy Oor and by asen.\n",
            "AUNEURK:\n",
            "O: I soni'd hat nobe?\n",
            "Wit trouch arrth?\n",
            "\n",
            "Huree in, Core?\n",
            "\n",
            "Wow taint, gead baver leoon lifsirnssis af ford.\n",
            "\n",
            "Os, oulddee, the mrot, pyath;\n",
            "Wo cathinf as be mave of, onthave king dyous uand alen is fartepe eet so'm'swead agarca wthe with lourts.\n",
            "\n",
            "To thoun? 'ld chelies wem of oucore\n",
            "Any ligs:\n",
            "Wef bet.\n",
            "Prort;\n",
            "This ther anw flassw st thetis harlore with, fre theayubyour ant that arnens; yell ange End;\n",
            "Tasuthearde, ard to coromakbave go altis; cot cin thy night an.\n",
            "Nund.\n",
            "\n",
            "KA thing nower, tilll ay liltins warn ond my sod cris leacte thans do bull of hured'dald's,\n",
            "To thouck.\n",
            "\n",
            "Whalst\n",
            "Hourllerdale:\n",
            "Ave\n",
            "Hold ow hat a with wher's tham weae, what mened?\n",
            "\n",
            "KETUFLESTEO:\n",
            "As tavight fi swland thour seed tuturnmeer hilment:\n",
            "Wy he non st ante prin tally and if what if I dam re onk I ace, py richt yo and t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYA5SgcdYtAW"
      },
      "source": [
        "### 01:24:25 Feedforward Layers of Transformer Block\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmWQqMBsYtAW"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bigramv2.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# n_head = 4\n",
        "# n_layer = 4\n",
        "# dropout = 0.0\n",
        "#-------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Download the dataset tiny Shakespear Dataset\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read it in to inspect it\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Length of dataset in characters: \", len(text))\n",
        "\n",
        "# Check all unique characters in the test\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(len(chars))\n",
        "\n",
        "\n",
        "# create a mapping for characters to integers\n",
        "stoi = {ch:i for i, ch in enumerate(chars)} # string to integer\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # ie take a string , output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # ie take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n",
        "\n",
        "\n",
        "# lets now encode the entire text dataset and store it into a torch.Tensor\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])\n",
        "\n",
        "# lets split the data into train and test datasets/validation\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "#dataloader Batches of chunks of data\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data # specify data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device) # when you load the data, make sure to move it to the device\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"one head of self attention\"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x) # (B,T,16)\n",
        "        q = self.query(x) # (B,T,16)\n",
        "        # compute attention scores ie affinities\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5  #(B,T,16) @ (B,16, T) --> (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        # perform weighted aggregation of values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size=head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer bloc communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sa(x)\n",
        "        x = self.ffwd(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self): # removing vocabsize since it's a global var\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # adding emedding to map from 65 to 32\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # tracks the position of the embedding\n",
        "        self.sa_head = MultiHeadAttention(4, n_embd//4) # ie 4 heads of 8-dimensional self-attention\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # remapping from 32 to 65\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape #decode bt from idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) #(B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T,C\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_head(x)\n",
        "        x = self.ffwd(x)\n",
        "        logits = self.lm_head(x)  # finding the prob of token+its position\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last step\n",
        "            logits = logits[:, -1, :] # from (B,T,C) to (B,C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sa,ple from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "losses = []  # List to store loss values\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "outputId": "8da3cb6b-dc2c-4358-b2c2-6df891e9de59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iiW6vlb9Jn0i"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting bigramv2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bigramv2.py"
      ],
      "metadata": {
        "outputId": "01374dc5-0855-429d-f723-beefeec6f534",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkahjLfnJn0u"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n",
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n",
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
            "step 0: train loss 4.1996, val loss 4.1995\n",
            "step 500: train loss 2.5993, val loss 2.6077\n",
            "step 1000: train loss 2.4629, val loss 2.4651\n",
            "step 1500: train loss 2.3974, val loss 2.3951\n",
            "step 2000: train loss 2.3297, val loss 2.3470\n",
            "step 2500: train loss 2.3018, val loss 2.3221\n",
            "step 3000: train loss 2.2828, val loss 2.2936\n",
            "step 3500: train loss 2.2495, val loss 2.2721\n",
            "step 4000: train loss 2.2435, val loss 2.2468\n",
            "step 4500: train loss 2.2286, val loss 2.2411\n",
            "\n",
            "Ba hil thill shat coo he hot mes fin.\n",
            "\n",
            "Cy hirad I four shat son yald hat lods guk- have ave lithr\n",
            "GLOull\n",
            "Wllld, with.\n",
            "\n",
            "BANTAUCHAR:\n",
            "I ork sak's willl eger aepale ganed ay wouce\n",
            "song thy in noduace being uliths tot upiord--mard meme the fles,\n",
            "Tharr, leanven-ty's thy on it in weancepte, digus I the souts pof ther cork, if woth' that he the bre tolf,\n",
            "An live:\n",
            "Mrins,\n",
            "What,\n",
            "Whorend;\n",
            "Ant to mal ry fa of thaw a tes tir, to tis argito-, setolf brntens?\n",
            "I' sit mem, thand yought is an he frifperend tepefurshith that.\n",
            "\n",
            "O:\n",
            "Wock of why he incivith kepy'st\n",
            "Turouxny; thave: wouch henote! st hen sow pompeet, frus' set, cfderord:\n",
            "Hall half\n",
            "The ont; Takourest of uns,\n",
            "May ther dist ther\n",
            "We\n",
            "Afoost wheys\n",
            "Werblage not the wrake if slond yous oncel wit sin you\n",
            "ilt angbe. Turlfa bard rly warsse\n",
            "Rt of ponw-t the mo: eor wedl, ont, ave.\n",
            "\n",
            "DULLARD OTHETORBERMMENEMNCEST: and Yof wog ans nownl hou sword alll thoud will-'s tolithe et bood pel fak,\n",
            "By bert muon.\n",
            "\n",
            "KAHOHS:\n",
            "I bro and bler.\n",
            "\n",
            "Rhis yest-then tonettba we fathan that andl tem.\n",
            "\n",
            "Yo, lesths light\n",
            "nome loid ovenotths ta ipll craint see: vo locous; to ay lien gates.\n",
            "That ly I thoust wok:\n",
            "Whangete ead gava efie tey I raiger theld' but uy Oor and by asen.\n",
            "AUNEURK:\n",
            "O: I soni'd hat nobe?\n",
            "Wit trouch arrth?\n",
            "\n",
            "Huree in, Core?\n",
            "\n",
            "Wow taint, gead baver leoon lifsirnssis af ford.\n",
            "\n",
            "Os, oulddee, the mrot, pyath;\n",
            "Wo cathinf as be mave of, onthave king dyous uand alen is fartepe eet so'm'swead agarca wthe with lourts.\n",
            "\n",
            "To thoun? 'ld chelies wem of oucore\n",
            "Any ligs:\n",
            "Wef bet.\n",
            "Prort;\n",
            "This ther anw flassw st thetis harlore with, fre theayubyour ant that arnens; yell ange End;\n",
            "Tasuthearde, ard to coromakbave go altis; cot cin thy night an.\n",
            "Nund.\n",
            "\n",
            "KA thing nower, tilll ay liltins warn ond my sod cris leacte thans do bull of hured'dald's,\n",
            "To thouck.\n",
            "\n",
            "Whalst\n",
            "Hourllerdale:\n",
            "Ave\n",
            "Hold ow hat a with wher's tham weae, what mened?\n",
            "\n",
            "KETUFLESTEO:\n",
            "As tavight fi swland thour seed tuturnmeer hilment:\n",
            "Wy he non st ante prin tally and if what if I dam re onk I ace, py richt yo and t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzoQfB__YtAW"
      },
      "source": [
        "### 01:26:48 Residual Connections\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBEqiizvYtAX"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile bigramv2.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "block_size = 8\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 1e-3\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "eval_iters = 200\n",
        "n_embd = 32\n",
        "# n_head = 4\n",
        "# n_layer = 4\n",
        "# dropout = 0.0\n",
        "#-------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Download the dataset tiny Shakespear Dataset\n",
        "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "\n",
        "# read it in to inspect it\n",
        "with open(\"input.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"Length of dataset in characters: \", len(text))\n",
        "\n",
        "# Check all unique characters in the test\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(len(chars))\n",
        "\n",
        "\n",
        "# create a mapping for characters to integers\n",
        "stoi = {ch:i for i, ch in enumerate(chars)} # string to integer\n",
        "itos = {i:ch for i, ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s] # ie take a string , output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # ie take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n",
        "\n",
        "\n",
        "# lets now encode the entire text dataset and store it into a torch.Tensor\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:100])\n",
        "\n",
        "# lets split the data into train and test datasets/validation\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "#dataloader Batches of chunks of data\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data # specify data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device) # when you load the data, make sure to move it to the device\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\"one head of self attention\"\"\"\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x) # (B,T,16)\n",
        "        q = self.query(x) # (B,T,16)\n",
        "        # compute attention scores ie affinities\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5  #(B,T,16) @ (B,16, T) --> (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        # perform weighted aggregation of values\n",
        "        v = self.value(x)\n",
        "        out = wei @ v\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"multiple heads of self-attention in parallel\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size=head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"a simple linear layer followed by a non-linearity\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer bloc communication followed by computation\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sa(x)\n",
        "        x = self.ffwd(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self): # removing vocabsize since it's a global var\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd) # adding emedding to map from 65 to 32\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd) # tracks the position of the embedding\n",
        "        self.sa_head = MultiHeadAttention(4, n_embd//4) # ie 4 heads of 8-dimensional self-attention\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size) # remapping from 32 to 65\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape #decode bt from idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) #(B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # T,C\n",
        "        x = tok_emb + pos_emb\n",
        "        x = self.sa_head(x)\n",
        "        x = self.ffwd(x)\n",
        "        logits = self.lm_head(x)  # finding the prob of token+its position\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last step\n",
        "            logits = logits[:, -1, :] # from (B,T,C) to (B,C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sa,ple from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "\n",
        "# create a pytorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "losses = []  # List to store loss values\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "outputId": "d962e1d9-84dd-44b0-c5f8-e249b88695f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EELFBCiQrQup"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting bigramv2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python bigramv2.py"
      ],
      "metadata": {
        "outputId": "00642bab-6f14-401e-9cda-b31320ac1b6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0lap_pvrQu1"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of dataset in characters:  1115394\n",
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n",
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n",
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n",
            "step 0: train loss 4.1996, val loss 4.1995\n",
            "step 500: train loss 2.5993, val loss 2.6077\n",
            "step 1000: train loss 2.4629, val loss 2.4651\n",
            "step 1500: train loss 2.3974, val loss 2.3951\n",
            "step 2000: train loss 2.3297, val loss 2.3470\n",
            "step 2500: train loss 2.3018, val loss 2.3221\n",
            "step 3000: train loss 2.2828, val loss 2.2936\n",
            "step 3500: train loss 2.2495, val loss 2.2721\n",
            "step 4000: train loss 2.2435, val loss 2.2468\n",
            "step 4500: train loss 2.2286, val loss 2.2411\n",
            "\n",
            "Ba hil thill shat coo he hot mes fin.\n",
            "\n",
            "Cy hirad I four shat son yald hat lods guk- have ave lithr\n",
            "GLOull\n",
            "Wllld, with.\n",
            "\n",
            "BANTAUCHAR:\n",
            "I ork sak's willl eger aepale ganed ay wouce\n",
            "song thy in noduace being uliths tot upiord--mard meme the fles,\n",
            "Tharr, leanven-ty's thy on it in weancepte, digus I the souts pof ther cork, if woth' that he the bre tolf,\n",
            "An live:\n",
            "Mrins,\n",
            "What,\n",
            "Whorend;\n",
            "Ant to mal ry fa of thaw a tes tir, to tis argito-, setolf brntens?\n",
            "I' sit mem, thand yought is an he frifperend tepefurshith that.\n",
            "\n",
            "O:\n",
            "Wock of why he incivith kepy'st\n",
            "Turouxny; thave: wouch henote! st hen sow pompeet, frus' set, cfderord:\n",
            "Hall half\n",
            "The ont; Takourest of uns,\n",
            "May ther dist ther\n",
            "We\n",
            "Afoost wheys\n",
            "Werblage not the wrake if slond yous oncel wit sin you\n",
            "ilt angbe. Turlfa bard rly warsse\n",
            "Rt of ponw-t the mo: eor wedl, ont, ave.\n",
            "\n",
            "DULLARD OTHETORBERMMENEMNCEST: and Yof wog ans nownl hou sword alll thoud will-'s tolithe et bood pel fak,\n",
            "By bert muon.\n",
            "\n",
            "KAHOHS:\n",
            "I bro and bler.\n",
            "\n",
            "Rhis yest-then tonettba we fathan that andl tem.\n",
            "\n",
            "Yo, lesths light\n",
            "nome loid ovenotths ta ipll craint see: vo locous; to ay lien gates.\n",
            "That ly I thoust wok:\n",
            "Whangete ead gava efie tey I raiger theld' but uy Oor and by asen.\n",
            "AUNEURK:\n",
            "O: I soni'd hat nobe?\n",
            "Wit trouch arrth?\n",
            "\n",
            "Huree in, Core?\n",
            "\n",
            "Wow taint, gead baver leoon lifsirnssis af ford.\n",
            "\n",
            "Os, oulddee, the mrot, pyath;\n",
            "Wo cathinf as be mave of, onthave king dyous uand alen is fartepe eet so'm'swead agarca wthe with lourts.\n",
            "\n",
            "To thoun? 'ld chelies wem of oucore\n",
            "Any ligs:\n",
            "Wef bet.\n",
            "Prort;\n",
            "This ther anw flassw st thetis harlore with, fre theayubyour ant that arnens; yell ange End;\n",
            "Tasuthearde, ard to coromakbave go altis; cot cin thy night an.\n",
            "Nund.\n",
            "\n",
            "KA thing nower, tilll ay liltins warn ond my sod cris leacte thans do bull of hured'dald's,\n",
            "To thouck.\n",
            "\n",
            "Whalst\n",
            "Hourllerdale:\n",
            "Ave\n",
            "Hold ow hat a with wher's tham weae, what mened?\n",
            "\n",
            "KETUFLESTEO:\n",
            "As tavight fi swland thour seed tuturnmeer hilment:\n",
            "Wy he non st ante prin tally and if what if I dam re onk I ace, py richt yo and t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpXDZmHVYtAX"
      },
      "source": [
        "### 01:32:51 Layernorm (and Its Relationship to Our Previous Batchnorm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8EnOXqUYtAX"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8wWe5n9YtAX"
      },
      "source": [
        "### 01:37:49 Scaling Up the Model! Creating a Few Variables, Adding Dropout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrcNj3JkYtAY"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KefVzbJRYtAY"
      },
      "source": [
        "### 01:42:39 Notes on Transformer: Encoder vs. Decoder vs. Both (?) Transformers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcPUoLk2YtAY"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHo3losqYtAY"
      },
      "source": [
        "### 01:46:22 Super Quick Walkthrough of nanoGPT, Batched Multi-Headed Self-Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09guJJaLYtAY"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLwtZn4DYtAY"
      },
      "source": [
        "### 01:48:53 Back to ChatGPT, GPT-3, Pretraining vs. Finetuning, RLHF\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKUxS2ZaYtAZ"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dQU3trTYtAZ"
      },
      "source": [
        "### 01:54:32 Conclusions"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}